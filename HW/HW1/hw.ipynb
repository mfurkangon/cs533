{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS533 HW1\n",
    "In this assignment, you will implement an information retrieval system using a combination of traditional\n",
    "retrieval methods and word embeddings. Specifically, you will be integrating TF-IDF and BM25 ranking\n",
    "methods with FastText embeddings. \n",
    "\n",
    "- Your task involves three different cases for using FastText: \n",
    "\n",
    "    (1) training a FastText model from scratch on the CISI dataset, \n",
    "\n",
    "    (2) using a pre-trained FastText model, and \n",
    "\n",
    "    (3) fine-tuning a pre-trained FastText model on the CISI dataset. \n",
    "    \n",
    "- You will rank documents for the given queries based on a combination of their traditional retrieval scores and the similarity between their embedding vectors. The performance of your retrieval system will be evaluated using the Mean Average Precision (MAP) metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Dataset Preprocessing\n",
    "\n",
    "* Load the CISI dataset from the provided CSV files. The dataset consists of:\n",
    "    \n",
    "    Documents: Represented by document id, title, and text.\n",
    "\n",
    "    Queries: List of natural language questions represented by query ids and texts.\n",
    "    \n",
    "    Ground Truth: Contains relevance judgments for query-document pairs.\n",
    "\n",
    "* Preprocess the dataset. This may include tokenizing the text, removing stopwords and punctuation marks, and lowercasing the text. Explain your preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from gensim.models import FastText, KeyedVectors\n",
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents:\n",
      "   Unnamed: 0  doc_id                                             title  \\\n",
      "0           0       1  18 Editions of the Dewey Decimal Classifications   \n",
      "1           1       2                                               NaN   \n",
      "2           2       3                                Two Kinds of Power   \n",
      "3           3       4        Systems Analysis of a University Library;    \n",
      "4           4       5                        A Library Management Game:   \n",
      "\n",
      "           author                                               text  \n",
      "0  Comaromi, J.P.     The present study is a history of the DEWEY...  \n",
      "1             NaN  This report is an analysis of 6300 acts of use...  \n",
      "2      Wilson, P.      The relationships between the organization...  \n",
      "3  Buckland, M.K.      The establishment of nine new universities...  \n",
      "4      Brophy, P.      Although the use of games in professional ...  \n",
      "\n",
      "Queries:\n",
      "   Unnamed: 0  query_id                                               text\n",
      "0           0         1  What problems and concerns are there in making...\n",
      "1           1         2  How can actually pertinent data, as opposed to...\n",
      "2           2         3  What is information science?  Give definitions...\n",
      "3           3         4  Image recognition and any other methods of aut...\n",
      "4           4         5  What special training will ordinary researcher...\n",
      "\n",
      "Ground Truth:\n",
      "   Unnamed: 0  query_id  doc_id\n",
      "0           0         1      28\n",
      "1           1         1      35\n",
      "2           2         1      38\n",
      "3           3         1      42\n",
      "4           4         1      43\n"
     ]
    }
   ],
   "source": [
    "# Define the paths\n",
    "base_path = os.getcwd()\n",
    "cisi_path = os.path.join(base_path, \"CISI\")\n",
    "\n",
    "# Load the datasets\n",
    "documents = pd.read_csv(os.path.join(cisi_path, \"documents.csv\"))\n",
    "queries = pd.read_csv(os.path.join(cisi_path, \"queries.csv\"))\n",
    "ground_truth = pd.read_csv(os.path.join(cisi_path, \"ground_truth.csv\"))\n",
    "\n",
    "# Print data summaries\n",
    "print(\"Documents:\")\n",
    "print(documents.head())\n",
    "print(\"\\nQueries:\")\n",
    "print(queries.head())\n",
    "print(\"\\nGround Truth:\")\n",
    "print(ground_truth.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mahmu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mahmu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mahmu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\mahmu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Ensure the input is a string, else return an empty string\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    # Removing punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenizing\n",
    "    tokens = word_tokenize(text)\n",
    "    # Removing stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    # Join tokens back into a single string\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  doc_id                                             title  \\\n",
      "0           0       1  18 Editions of the Dewey Decimal Classifications   \n",
      "1           1       2                                               NaN   \n",
      "2           2       3                                Two Kinds of Power   \n",
      "3           3       4        Systems Analysis of a University Library;    \n",
      "4           4       5                        A Library Management Game:   \n",
      "\n",
      "           author                                               text  \\\n",
      "0  Comaromi, J.P.     The present study is a history of the DEWEY...   \n",
      "1             NaN  This report is an analysis of 6300 acts of use...   \n",
      "2      Wilson, P.      The relationships between the organization...   \n",
      "3  Buckland, M.K.      The establishment of nine new universities...   \n",
      "4      Brophy, P.      Although the use of games in professional ...   \n",
      "\n",
      "                                      processed_text  \n",
      "0  present study history dewey decimal classifica...  \n",
      "1  report analysis 6300 act use 104 technical lib...  \n",
      "2  relationship organization control writing orga...  \n",
      "3  establishment nine new university 1960s provok...  \n",
      "4  although use game professional education becom...  \n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing to the text column\n",
    "documents['processed_text'] = documents['text'].apply(preprocess_text)\n",
    "\n",
    "# View the processed data\n",
    "print(documents.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Unnamed: 0  doc_id                      title             author text  \\\n",
      "790         790     791  Progress in Documentation  Fairthorne, R.A.   NaN   \n",
      "\n",
      "    processed_text  \n",
      "790                 \n"
     ]
    }
   ],
   "source": [
    "print(documents[documents['processed_text'] == \"\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 2: Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    return word_tokenize(text) if isinstance(text, str) else []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Training FastText:\n",
    "Use the gensim library to train a FastText model on the CISI dataset. Ensure that both the text from the queries and the documents are used for the training process. This approach will help the model learn the specific vocabulary and context present in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize documents\n",
    "document_sentences = documents['processed_text'].apply(tokenize_text).tolist()\n",
    "\n",
    "# Tokenize queries\n",
    "query_sentences = queries['text'].apply(preprocess_text).apply(tokenize_text).tolist()\n",
    "\n",
    "# Combine sentences from both documents and queries\n",
    "all_sentences = document_sentences + query_sentences\n",
    "\n",
    "# Flatten nested lists and remove empty sentences\n",
    "all_sentences = [\" \".join(sentence) for sentence in all_sentences if sentence]\n",
    "\n",
    "# Save tokenized sentences to a training text file\n",
    "training_file_path = \"fasttext_cisi_training.txt\"\n",
    "with open(training_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for sentence in all_sentences:\n",
    "        f.write(sentence + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText model trained and saved to fasttext_cisi_model.bin\n"
     ]
    }
   ],
   "source": [
    "# Train FastText model using the fasttext library\n",
    "fasttext_model = fasttext.train_unsupervised(\n",
    "    input=training_file_path,  # Path to the training file\n",
    "    model=\"skipgram\",          # Skip-gram model (use 'cbow' for CBOW)\n",
    "    dim=300,                   # Dimensionality of the embeddings\n",
    "    ws=5,                      # Context window size\n",
    "    minCount=2,                # Minimum word count threshold\n",
    "    epoch=100                   # Number of training epochs\n",
    ")\n",
    "\n",
    "# Save the trained FastText model in binary format\n",
    "model_save_path = \"fasttext_cisi_model.bin\"\n",
    "fasttext_model.save_model(model_save_path)\n",
    "\n",
    "print(f\"FastText model trained and saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'example': [ 0.3601462  -0.03513972 -0.2338145  -0.07723407  0.3736553  -0.16462587\n",
      " -0.07055941  0.05074128 -0.1485961  -0.08871047  0.2874292  -0.22264093\n",
      " -0.00766258  0.39132214  0.3407526   0.08849433 -0.47778815  0.13766488\n",
      " -0.38083586  0.01022877  0.06978649  0.25788116  0.22212768 -0.0748836\n",
      " -0.10919072 -0.04937812 -0.35594967  0.11519236  0.04441971  0.20399311\n",
      " -0.11428595 -0.01238429 -0.00183018 -0.10965278  0.02442933  0.14038679\n",
      "  0.3982956   0.1756552  -0.02610425 -0.38572773 -0.11633113 -0.25150946\n",
      "  0.44252944 -0.40143698  0.23332897  0.77471334  0.18574254  0.20379612\n",
      " -0.18797651  0.6479729  -0.46667814 -0.16886805  0.06523523  0.13803321\n",
      " -0.3055783   0.27377272 -0.4933838   0.28193444 -0.19928853  0.00812812\n",
      "  0.09831779  0.19868183 -0.2364418  -0.15905543 -0.05473196 -0.46689537\n",
      " -0.25492218 -0.42084703  0.6647783  -0.07065783  0.36880407 -0.17253518\n",
      " -0.4677974  -0.3538534  -0.2766774  -0.2462121  -0.01505099 -0.03724478\n",
      "  0.10972012 -0.01472945 -0.25200185 -0.275613   -0.05122358  0.14948127\n",
      "  0.42351785 -0.11554842  0.24301839  0.2493305   0.37146607 -0.1522041\n",
      "  0.1081023   0.01547329  0.27113068 -0.6469158   0.28140572  0.09867627\n",
      "  0.1322379  -0.0992201  -0.20420025  0.02030023  0.19892089 -0.23865223\n",
      "  0.5683499  -0.4430133   0.3543729  -0.3167996  -0.11936934  0.14147407\n",
      "  0.19172393 -0.14092775 -0.11959523 -0.4193221  -0.19321562  0.1603325\n",
      "  0.04908666  0.34131736 -0.21383633  0.64884263 -0.3536806  -0.12138411\n",
      "  0.08582566  0.00597092 -0.1358551  -0.07940393 -0.25861636 -0.4938685\n",
      "  0.09307824 -0.1793208  -0.0703558   0.30345634 -0.4854833  -0.52158046\n",
      "  0.18251434 -0.29628733  0.66791934 -0.16035332 -0.433478    0.155793\n",
      " -0.38583317  0.20502555  0.13654402 -0.13876222  0.15810308  0.4792495\n",
      "  0.11718672 -0.40286508 -0.09120765  0.35727867  0.10547805  0.16250381\n",
      " -0.1166505   0.7329725  -0.4757026  -0.25860727 -0.003885   -0.15237077\n",
      " -0.09701362  0.1839518   0.23931748  0.28983748  0.32428497  0.36774963\n",
      "  0.09532096  0.43145514  0.22529589 -0.32766214 -0.44406223  0.00314515\n",
      "  0.19283094  0.27222767 -0.11070877  0.28744143 -0.09637853 -0.19993666\n",
      " -0.05797265  0.2309249  -0.01636638 -0.01345235  0.3777316   0.06208405\n",
      "  0.45967817 -0.06149631  0.11428131 -0.11231811 -0.36442322 -0.18569563\n",
      "  0.25028703  0.06539202  0.10271113 -0.01405958 -0.1073686   0.3487339\n",
      " -0.16391405  0.22890104  0.17487404 -0.5118182   0.22470556 -0.3135929\n",
      "  0.12440259 -0.13129422 -0.38654587 -0.254787   -0.33719942  0.40852028\n",
      " -0.1449493   0.24930552 -0.1858848   0.5356161  -0.48610142 -0.02685413\n",
      "  0.72406805  0.0660298   0.11972478 -0.6568944   0.2670186  -0.05130381\n",
      " -0.5438148   0.32994992  0.31023684 -0.25372455  0.647694   -0.06768374\n",
      " -0.0672932   0.37645623  0.3609411   0.06178334 -0.00217344  0.35390776\n",
      "  0.10669678  0.24217059  0.05976285  0.02332169  0.10916906  0.06551785\n",
      " -0.47069135 -0.3551579  -0.3822882  -0.2907575   0.07758134 -0.28176114\n",
      "  0.35856122 -0.13539156  0.10901983 -0.12038374 -0.3121059   0.00950311\n",
      "  0.09153354 -0.07402787 -0.3629971  -0.37683022  0.03980704  0.14823553\n",
      " -0.120245    0.23374502  0.27144185  0.08131541 -0.07050696 -0.06826988\n",
      " -0.0147989  -0.4880918   0.15886572  0.4104663   0.604652    0.01529929\n",
      " -0.3665459   0.00700402 -0.42886496  0.17411797  0.03236751 -0.04410768\n",
      "  0.06658487 -0.15858372 -0.00373892 -0.35860336  0.03705996 -0.07478921\n",
      " -0.24567199  0.29030383  0.5895344   0.01717301  0.06548133 -0.19912094\n",
      " -0.16005921 -0.36883786 -0.23428705 -0.09785769  0.7148463  -0.17691934\n",
      " -0.21415286  0.21237881 -0.31133375  0.25857255 -0.00722372  0.14441067\n",
      " -0.10324769  0.01817599 -0.10840786  0.20239292  0.27305302 -0.0579708 ]\n"
     ]
    }
   ],
   "source": [
    "# Access vector representations\n",
    "word_vector = fasttext_model.get_word_vector('example')  # Vector for the word \"example\"\n",
    "print(\"Vector for 'example':\", word_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.5058606266975403, 'ample'),\n",
       " (0.45378610491752625, 'counterexample'),\n",
       " (0.3805534541606903, 'illustrated'),\n",
       " (0.3674928843975067, 'illustrate'),\n",
       " (0.3653709292411804, 'illustration')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_model.get_nearest_neighbors('example', k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.4939620792865753, 'reporter'),\n",
       " (0.4387853443622589, 'reported'),\n",
       " (0.4237591624259949, 'reporting'),\n",
       " (0.33701103925704956, 'calbpc'),\n",
       " (0.32045799493789673, 'ugc')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_model.get_nearest_neighbors('report', k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Using a Pre-trained FastText Model: \n",
    "Instead of starting from scratch, you can leverage pre-existing knowledge by using a pre-trained FastText model. Download a pre-trained FastText model, such as “cc.en.300.bin” (English), from the FastText website or using the fasttext library. The pre-trained model can be loaded using the gensim library for direct use in your retrieval system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained FastText model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Path to the pre-trained FastText model\n",
    "pretrained_model_path = os.path.join(base_path, \"cc.en.300.bin\")\n",
    "\n",
    "# Load the pre-trained FastText model\n",
    "pretrained_fasttext_model = fasttext.load_model(pretrained_model_path)\n",
    "\n",
    "print(\"Pre-trained FastText model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'example': [-3.01899910e-02  1.67307898e-03 -3.39188091e-02  1.29165754e-04\n",
      " -3.39024775e-02 -3.52627262e-02  5.44663481e-02 -2.15502288e-02\n",
      "  1.57393347e-02 -5.50850853e-03 -9.77861509e-03  6.96822815e-03\n",
      "  1.34404376e-02  4.04827148e-02 -5.77299595e-02  2.67399456e-02\n",
      "  4.28873971e-02  1.72743984e-02  5.14067225e-02  4.15806361e-02\n",
      " -3.46253510e-03 -4.39561009e-02  4.55061607e-02 -4.61385176e-02\n",
      " -6.82864487e-02 -1.10961404e-02  1.33144371e-02  2.14999523e-02\n",
      "  8.21126904e-03 -5.76011557e-03  1.62116960e-02  6.52960828e-03\n",
      "  7.23410025e-03 -5.48320338e-02 -1.13268523e-02 -9.41580534e-03\n",
      "  3.99618335e-02 -5.51603436e-02 -4.69672195e-05 -5.19470498e-02\n",
      " -3.15293521e-02 -4.06791782e-03 -5.40495440e-02 -1.99173968e-02\n",
      " -8.28304701e-03  4.20339815e-02  2.26341262e-02 -1.23577183e-02\n",
      "  1.77250840e-02  2.66364366e-02  2.01242566e-02  1.41719412e-02\n",
      " -4.94768023e-02  3.80847923e-04  1.61610469e-02 -3.24339680e-02\n",
      " -5.72527312e-02 -1.43544767e-02 -1.18667241e-02 -3.18274871e-02\n",
      " -6.09051716e-03 -1.75512806e-02 -1.88941509e-02  6.57918751e-02\n",
      " -1.70362704e-02 -2.38775145e-02  1.10819004e-02 -1.46602560e-02\n",
      " -3.17999572e-02 -1.35541838e-02  9.88431554e-03  1.95524376e-02\n",
      "  8.56627128e-04  2.85380837e-02  1.19605416e-03 -7.53230369e-03\n",
      "  1.82464644e-02  1.79416984e-02  2.20004246e-02  1.42588979e-02\n",
      " -1.28483716e-02  1.68150403e-02 -5.26246391e-02  6.04731254e-02\n",
      " -3.07555180e-02  2.20271456e-03  4.51202802e-02 -2.25942582e-02\n",
      "  8.83257855e-03  5.92258088e-02 -1.49658136e-02  1.42201185e-02\n",
      "  5.39260916e-02 -3.98425572e-03 -3.96032594e-02  3.12304823e-03\n",
      "  5.90320230e-02  3.23439157e-03  1.42818056e-02 -6.75218878e-03\n",
      " -2.45296024e-02  3.40816006e-03  4.59559560e-02 -1.39415951e-03\n",
      "  2.04592310e-02  1.62594113e-02 -8.37885309e-03 -3.22134793e-02\n",
      " -4.18445878e-02 -4.48241830e-04  2.27591917e-02  1.05035067e-01\n",
      " -6.87445840e-03  2.30047964e-02 -1.66694373e-02 -3.09513789e-02\n",
      " -2.53222641e-02 -3.28900246e-03  1.32123509e-03  1.55036040e-02\n",
      " -3.37475166e-02 -3.57205211e-03  8.70069675e-03  1.75750591e-02\n",
      " -1.24017140e-02 -2.35731713e-02 -1.88518390e-02  2.83566769e-02\n",
      " -2.68664323e-02  2.52381712e-02  8.98338947e-03  1.00622699e-03\n",
      "  8.73946585e-03  3.65372710e-02  6.62934734e-03  1.60806179e-02\n",
      " -4.48645800e-02 -2.15704460e-03  2.22848281e-02  6.43435214e-03\n",
      "  9.87622142e-02  2.15436295e-02  3.47205102e-02  3.54047641e-02\n",
      "  4.50347364e-03  2.25602351e-02 -1.13495044e-01  4.40410338e-03\n",
      "  4.64368984e-03 -2.14721486e-02 -2.13853847e-02  1.65167563e-02\n",
      "  1.68756023e-03  1.81931034e-02 -2.40316298e-02 -5.71350288e-03\n",
      "  8.29948485e-02 -2.37549655e-02  4.76049557e-02  3.42812315e-02\n",
      " -5.68080246e-02  2.86809877e-02 -1.83667410e-02 -1.65337063e-02\n",
      "  1.88965537e-02 -6.54991530e-03 -1.30027067e-02  8.50359164e-03\n",
      "  9.80426185e-03  8.63667112e-03 -5.43191191e-03 -2.04030462e-02\n",
      "  7.01134354e-02 -3.55278999e-02 -3.87942046e-03  1.43057987e-04\n",
      " -5.89617901e-03 -9.47824679e-03 -2.04912517e-02 -2.75525413e-02\n",
      "  8.56543705e-03  7.33267590e-02 -1.36254393e-02  7.35934302e-02\n",
      " -4.66546882e-03  4.85000014e-02 -4.88026720e-03 -1.31496899e-02\n",
      "  3.62132043e-02  2.17573028e-02 -6.75447471e-03 -2.76161060e-02\n",
      "  2.80781966e-02  2.22665370e-02  3.42924632e-02  7.54242670e-03\n",
      "  1.63343605e-02 -1.64738949e-02 -1.74708981e-02  1.07365660e-01\n",
      " -2.19095685e-02  2.00895593e-02 -1.74494721e-02  8.74210149e-03\n",
      " -9.34810191e-03 -1.11030284e-02  7.86592215e-02 -4.37761750e-03\n",
      "  9.68915131e-03  4.93756309e-02 -1.01289777e-02  1.30043291e-02\n",
      "  1.04785800e-01  1.78876296e-02  1.05670956e-03  1.08829699e-04\n",
      "  2.04011574e-02  4.18889150e-02  5.21203410e-03  6.15003370e-02\n",
      "  1.49727371e-02  3.83200264e-03  2.79908534e-02  3.86267006e-02\n",
      " -1.05609549e-02 -4.45089713e-02  4.81903628e-02 -1.21327331e-02\n",
      "  2.27318462e-02  3.92154232e-03  6.89725205e-03  3.70222554e-02\n",
      "  4.04998809e-02  2.40451097e-02 -1.56110208e-02  4.67110937e-03\n",
      "  1.15064029e-02 -2.48153806e-02  2.46032383e-02  2.57952884e-02\n",
      " -2.33337190e-02  6.96498062e-03 -5.69340289e-02 -2.25956216e-02\n",
      "  9.39001665e-02  1.69412531e-02 -5.67482132e-03 -2.23616920e-02\n",
      "  2.57752202e-02 -4.43150609e-04  5.61242551e-03  3.46367201e-03\n",
      "  5.75484149e-02 -4.34293225e-02  2.45434027e-02  1.87426098e-02\n",
      " -2.88753491e-02 -4.59794402e-02  2.81588417e-02 -1.56906452e-02\n",
      "  9.10069980e-03  4.89695296e-02 -1.83173083e-02 -1.64116398e-02\n",
      " -7.92704523e-02  3.63688245e-02  3.72983143e-02  9.86254541e-04\n",
      "  4.70681004e-02 -4.05500643e-02  9.27161425e-04 -3.91272344e-02\n",
      "  2.35815831e-02 -4.02348414e-02 -7.46685788e-02 -2.76269224e-02\n",
      " -7.24740839e-03 -3.28706168e-02 -4.25594440e-03  2.08717268e-02\n",
      " -5.23702800e-03  1.65620688e-02 -6.25159591e-02 -1.05047300e-02\n",
      " -1.36466315e-02  2.26716022e-03 -1.56764835e-02 -8.22888501e-03\n",
      "  1.25319045e-02 -4.97945249e-02  6.70799613e-02  2.40621828e-02\n",
      " -4.89284005e-03  1.26976427e-02 -2.12681945e-02 -2.28267461e-02\n",
      " -8.61641113e-03  2.78180651e-02  3.45428102e-03  2.85061654e-02]\n"
     ]
    }
   ],
   "source": [
    "word_vector = pretrained_fasttext_model.get_word_vector('example')\n",
    "print(f\"Vector for 'example': {word_vector}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.8356781601905823, 'instance'),\n",
       " (0.7126652002334595, 'example.In'),\n",
       " (0.6859133839607239, 'exmaple'),\n",
       " (0.6804730296134949, 'example.The'),\n",
       " (0.6717150211334229, 'example.For')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find similar words\n",
    "pretrained_fasttext_model.get_nearest_neighbors('example', k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.7258307337760925, 'reports'),\n",
       " (0.6820427775382996, 'report.It'),\n",
       " (0.6762813925743103, 'report.The'),\n",
       " (0.6462412476539612, 'report.In'),\n",
       " (0.627001941204071, 'report.But')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_fasttext_model.get_nearest_neighbors('report', k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Fine-tuning a Pre-trained FastText Model: \n",
    "Further improve a pre-trained FastText model by finetuning it on the CISI dataset. This involves continuing the training process using the text from both the queries and the documents, allowing the model to adapt and better capture the characteristics and domain-specific vocabulary of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned FastText model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune the model on the CISI dataset\n",
    "finetuned_model = fasttext.train_unsupervised(\n",
    "    training_file_path, model=\"skipgram\",  # Use the Skip-gram approach\n",
    "    dim=300,                              # Keep the same dimension as the pre-trained model\n",
    "    ws=5,                                 # Window size\n",
    "    epoch=100,                             # Number of epochs for fine-tuning\n",
    "    lr=0.05                               # Learning rate\n",
    ")\n",
    "\n",
    "# Save the fine-tuned model\n",
    "finetuned_model.save_model(\"finetuned_fasttext_cisi.bin\")\n",
    "print(\"Fine-tuned FastText model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuned FastText model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Path to the pre-trained FastText model\n",
    "finetuned_model_path = os.path.join(base_path, \"finetuned_fasttext_cisi.bin\")\n",
    "\n",
    "# Load the pre-trained FastText model\n",
    "finetuned_fasttext_model = fasttext.load_model(finetuned_model_path)\n",
    "\n",
    "print(\"Finetuned FastText model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'example': [ 0.15368047 -0.02703001 -0.21425845 -0.11544652  0.25131127  0.06092939\n",
      "  0.02180931  0.09665485  0.06949833  0.11939456 -0.03517433  0.33522272\n",
      "  0.00855149  0.39514363  0.10516681  0.18342508  0.34751529 -0.09838767\n",
      "  0.11537067  0.40186012  0.03177984 -0.10024697  0.03056737  0.11176201\n",
      "  0.36433935 -0.20943132 -0.02506921  0.06632397 -0.25563937  0.05850577\n",
      " -0.19495898 -0.21565342  0.07011463  0.0396662  -0.13288555  0.5057059\n",
      "  0.16477966  0.11597671  0.14403333 -0.3003128  -0.01222487  0.03619796\n",
      " -0.04004008 -0.00271747  0.11331759  0.3749598   0.06666801  0.29852435\n",
      "  0.09872383  0.08984828 -0.20170486 -0.12968232  0.29882678  0.04643991\n",
      "  0.3801889  -0.08765279 -0.18479271  0.30083615  0.15751004 -0.05213655\n",
      " -0.16943225  0.02581784  0.07663221  0.06882079 -0.46061513  0.18307348\n",
      " -0.00190935 -0.5215184   0.202297    0.08151408 -0.0895128  -0.46907824\n",
      " -0.04973831  0.17032252 -0.10496243  0.21327695  0.02070305  0.02749459\n",
      "  0.05474547 -0.08357549  0.16588694  0.19286351 -0.21116924 -0.16939704\n",
      "  0.3160456  -0.23303464 -0.1775702   0.1843978   0.08628587 -0.20689037\n",
      " -0.37264526  0.23031646  0.29120967  0.08843353  0.19949089  0.5022844\n",
      " -0.55328816  0.45976794 -0.00587401 -0.46395332  0.3796416   0.16268672\n",
      " -0.22211725 -0.11150791  0.14038663  0.21353261  0.07251895 -0.06386314\n",
      "  0.3437244  -0.16458753  0.24167626 -0.0261285   0.15497631 -0.07808167\n",
      " -0.03605732  0.03819414  0.2909502   0.14368509  0.2807587  -0.42946562\n",
      " -0.06353793  0.08184934 -0.1862074   0.15239815 -0.00908898 -0.14556159\n",
      " -0.19353543 -0.12071535  0.1694501   0.01494335 -0.2519887  -0.17845084\n",
      " -0.01525055  0.27555305 -0.19330654 -0.14248143  0.25806886  0.08075252\n",
      "  0.11914324 -0.19320334 -0.05151255  0.11032608 -0.29849178  0.5675509\n",
      "  0.27936277 -0.22793742  0.16646892 -0.32467473  0.00261404  0.02574896\n",
      " -0.31316894 -0.11454158 -0.25144222  0.14556159  0.2721441   0.05881364\n",
      " -0.18463835  0.34062344 -0.12141995  0.3195433   0.2079145   0.01824993\n",
      " -0.0765987  -0.15954155 -0.25591862  0.05141218 -0.1158233  -0.08299051\n",
      " -0.00428118  0.0793793  -0.36653838 -0.11234608  0.11632151 -0.11033739\n",
      " -0.30219695 -0.44568953  0.0786515  -0.5232084  -0.2655635  -0.12243079\n",
      " -0.06546413  0.37028703  0.20320988  0.00964999 -0.07190453  0.01148251\n",
      "  0.04466231 -0.0078432  -0.20952813  0.1163611   0.00215962 -0.51971984\n",
      "  0.4196543   0.31904745 -0.0026029  -0.0771549   0.14754333  0.11132426\n",
      "  0.2568572  -0.2948725   0.27210417  0.13803853 -0.0304546   0.06389458\n",
      "  0.29180306  0.34636068  0.0574307   0.1332489  -0.6407768  -0.04221565\n",
      "  0.2103993   0.6861918  -0.16367888  0.21552408  0.07230171  0.00116459\n",
      "  0.7032654  -0.61542624 -0.03694423 -0.00167829  0.60451895 -0.54924333\n",
      "  0.01919367 -0.7405899  -0.01675168  0.13161294  0.35107803  0.20543793\n",
      " -0.33380905  0.29573622 -0.09007774 -0.23914558  0.03871286 -0.18415053\n",
      " -0.13556759 -0.05155383 -0.09309579 -0.33905974 -0.18588462  0.42839676\n",
      " -0.03271186  0.1360342  -0.27074364  0.49422345 -0.16167845 -0.55685914\n",
      " -0.34599406 -0.08477594 -0.42072877  0.44994816 -0.22138238 -0.05747611\n",
      " -0.21471648 -0.1654382   0.21350299 -0.24055421  0.12740673  0.4528574\n",
      " -0.14504556  0.00140992 -0.06306266 -0.05258539 -0.0840274   0.21798345\n",
      " -0.12543602 -0.0323564  -0.03489152 -0.05844508  0.36334834 -0.48300773\n",
      " -0.23121333 -0.08794924 -0.03522415  0.21847835  0.3226616   0.18055405\n",
      " -0.21741569 -0.30811572 -0.53749883 -0.14669925  0.02236676  0.00219448\n",
      " -0.00350406  0.38368788  0.23954354 -0.10710501  0.00515903  0.15518297\n",
      " -0.07546517  0.5306188  -0.07200307  0.27052623  0.23534052  0.4387976\n",
      "  0.09817229  0.13098754  0.01722683  0.04713468  0.21458869 -0.12941147]\n"
     ]
    }
   ],
   "source": [
    "word_vector = finetuned_fasttext_model.get_word_vector('example')\n",
    "print(f\"Vector for 'example': {word_vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.363446980714798, 'illustration'),\n",
       " (0.3617458641529083, 'illustrated'),\n",
       " (0.35683703422546387, 'illustrate'),\n",
       " (0.3160126209259033, 'illustrative'),\n",
       " (0.2687340974807739, 'coordination')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find similar words\n",
    "finetuned_fasttext_model.get_nearest_neighbors('example', k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.4687941372394562, 'reported'),\n",
       " (0.4027373492717743, 'reporting'),\n",
       " (0.2799728512763977, 'study'),\n",
       " (0.2762048840522766, 'colorado'),\n",
       " (0.26580870151519775, 'federal')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find similar words\n",
    "finetuned_fasttext_model.get_nearest_neighbors('report', k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Retrieval Task\n",
    "In this part, you will focus on the core of information retrieval: fetching relevant documents based on a user's\n",
    "query. The challenge lies in effectively combining traditional retrieval methods with modern word embeddings\n",
    "to enhance the accuracy of the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Embedding Computation: \n",
    "Implement a method to derive the embedding for an input (a query or a document) by averaging the embeddings of its constituent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embedding(sentence, model):\n",
    "    # Tokenize the input sentence\n",
    "    words = tokenize_text(sentence)\n",
    "    \n",
    "    # Retrieve embeddings for words in the model's vocabulary\n",
    "    word_vectors = []\n",
    "    for word in words:\n",
    "        if word in model:  # Check if the word is in the model's vocabulary\n",
    "            word_vectors.append(model[word])\n",
    "    \n",
    "    if not word_vectors:\n",
    "        # If no words are found in the model, return a zero vector\n",
    "        return np.zeros(model.get_dimension())\n",
    "    \n",
    "    # Compute the average of the embeddings\n",
    "    return np.mean(word_vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Embedding: [ 0.14987512  0.06952221 -0.06957391 -0.05053607  0.31149802 -0.11628633\n",
      " -0.05789592  0.09997884  0.10218373 -0.1806168   0.00511498  0.29726592\n",
      " -0.1846541  -0.05025312 -0.06120212  0.22680569 -0.01971935  0.18904953\n",
      "  0.12363468 -0.08691324  0.3065855   0.254293    0.07490369  0.07381604\n",
      "  0.00150687 -0.07969659  0.01648275  0.00492999  0.22681278 -0.19567622\n",
      "  0.2346856  -0.18983017 -0.01651494 -0.12024503 -0.11877542  0.02123858\n",
      "  0.28430143  0.1665491   0.03157415  0.12296946  0.04768892  0.14323549\n",
      " -0.1192183   0.33908176  0.13214438 -0.00437745 -0.08516368 -0.09647523\n",
      " -0.17207664  0.2282944   0.27714694 -0.05932504  0.14826377  0.05357199\n",
      " -0.04046134 -0.2120988  -0.25026935  0.02721096  0.27870777  0.17933954\n",
      "  0.1261993  -0.09355965 -0.03809638  0.2521343   0.16309899  0.24700512\n",
      "  0.08869053  0.08387133  0.10193145 -0.20604278  0.09603649 -0.02020621\n",
      " -0.04760153  0.20926857 -0.2605013   0.19680268  0.2038908  -0.00865525\n",
      " -0.1350343   0.01701168 -0.05053665 -0.03572116 -0.16526228  0.06755295\n",
      "  0.06866846 -0.03603804 -0.30500382 -0.1032274  -0.07822448 -0.12239271\n",
      "  0.10706685 -0.07642729  0.09450564 -0.01575143  0.16997582  0.09965806\n",
      "  0.00540684  0.04541032  0.21602917 -0.07558857  0.18394038 -0.09114119\n",
      " -0.17603733 -0.1019166   0.05277579  0.06585327  0.09865976  0.07023958\n",
      "  0.19180888  0.20694941 -0.18672742  0.4748459   0.09012023 -0.23108755\n",
      " -0.04289635 -0.11445896  0.3722748  -0.01980824  0.14324443  0.17204534\n",
      " -0.38751552 -0.0026528  -0.22362924  0.13791652 -0.0723943  -0.21247227\n",
      " -0.02696301  0.38935396 -0.06972013  0.05208054 -0.15070191 -0.09561408\n",
      "  0.0558148   0.24565366  0.28811887 -0.04105085 -0.06709128  0.04800268\n",
      " -0.03129027 -0.03521119  0.12786487 -0.13925785  0.09831081  0.12616885\n",
      " -0.24159391 -0.27051204  0.08812956 -0.188054    0.06956079  0.14138308\n",
      " -0.18593435  0.069837   -0.26437464 -0.00764732  0.08449443  0.33965883\n",
      " -0.377525    0.19046277  0.21905781  0.10117098 -0.12984204  0.16922344\n",
      "  0.25964087  0.00925218  0.22414504 -0.13013852  0.04148002 -0.16040264\n",
      "  0.27358    -0.11605445 -0.24452288  0.04656663  0.1334644  -0.21106999\n",
      " -0.02607046 -0.21622142  0.09378818  0.0862568  -0.14796828  0.0615567\n",
      " -0.07782096 -0.44132838  0.088911   -0.1058366  -0.0540601   0.17093508\n",
      " -0.15043661  0.09183767 -0.1783065   0.21802491 -0.12115313 -0.17092198\n",
      "  0.22547852 -0.36052078  0.09346895  0.1532764  -0.3506204  -0.14332387\n",
      " -0.09637883 -0.14438048  0.34831873 -0.02529319  0.08014264  0.01579173\n",
      "  0.02767271  0.36292034  0.03957569  0.1415236   0.31789994  0.00342086\n",
      "  0.12771383  0.06965632  0.10470255  0.18572435  0.15901838 -0.22085395\n",
      " -0.0694299   0.24352807  0.17486551  0.09152418  0.16121744 -0.051642\n",
      "  0.18273945 -0.18097624 -0.05444437 -0.02006709  0.03896661  0.10448399\n",
      " -0.01876542  0.3022915   0.11771759  0.17983787  0.00080432 -0.12895024\n",
      " -0.0196485   0.04111802 -0.2952196  -0.20829508  0.036084    0.02465597\n",
      "  0.18241721  0.09821896  0.1353707   0.10542062 -0.05647578 -0.02543719\n",
      " -0.11115091 -0.10511766 -0.12554099 -0.08298884  0.18234907  0.00590192\n",
      "  0.05098803  0.02274317 -0.10233906 -0.14173637  0.25088271  0.2218627\n",
      "  0.17094484  0.01573419 -0.021579   -0.16760597  0.03461397 -0.04886381\n",
      "  0.12381086 -0.11465296  0.05416055 -0.06750702 -0.12806605 -0.11132239\n",
      " -0.1806517   0.28866062  0.12193052 -0.143747    0.02177826  0.06545617\n",
      " -0.07300103 -0.32482752  0.01200816  0.06502654 -0.10736287 -0.11617344\n",
      "  0.19264622 -0.06395724  0.08714613 -0.03847942  0.05792923  0.20376763\n",
      " -0.11308654 -0.03343728  0.07966123  0.3033267   0.06020415 -0.18318088\n",
      " -0.28980443 -0.18411772  0.11774752 -0.29666278  0.2483813   0.08648816]\n",
      "Document Embedding: [ 2.36038446e-01  1.52528182e-01 -6.98917508e-02  3.77696976e-02\n",
      " -8.45673159e-02  1.07813172e-01 -5.61384708e-02  5.36269583e-02\n",
      "  1.08461432e-01 -4.58587967e-02  2.33523920e-01  1.54042646e-01\n",
      "  7.46133849e-02 -9.40782279e-02 -2.94182152e-02  9.90370661e-02\n",
      "  9.47804749e-02  6.71827123e-02  5.38180619e-02 -2.38013081e-02\n",
      "  2.79392332e-01 -1.86097459e-03  1.90140694e-01 -1.22547470e-01\n",
      " -2.08412454e-01 -7.73009285e-02  1.62763938e-01 -3.66904330e-03\n",
      " -5.78894317e-02  9.57657620e-02  1.47410661e-01 -1.61867023e-01\n",
      " -3.27986851e-02  2.33885925e-02  7.12322444e-02 -1.21307231e-01\n",
      "  2.99053520e-01  1.69398099e-01 -2.93819845e-01 -1.56019747e-01\n",
      "  1.90112054e-01  7.22950026e-02 -1.69546291e-01  2.13860571e-01\n",
      "  9.32991579e-02 -7.65044838e-02 -8.85139480e-02 -5.59060201e-02\n",
      " -3.58535498e-02  3.15935522e-01  1.19490042e-01 -1.45784736e-01\n",
      "  1.64900333e-01 -3.86134088e-02  7.90712535e-02  3.44753414e-02\n",
      " -9.55003276e-02 -3.42168100e-02 -3.33634168e-02 -5.18815480e-02\n",
      "  2.81222433e-01  3.42775695e-02  5.03381751e-02 -2.65378002e-02\n",
      "  3.82997304e-01  2.21372217e-01  1.07709328e-02 -4.61764708e-02\n",
      "  2.77482998e-02 -1.48310810e-01  8.48440453e-02 -1.52599290e-01\n",
      "  1.15457542e-01  1.29436970e-01  6.03628792e-02  1.20061189e-01\n",
      "  1.04588605e-01 -1.47756234e-01 -1.67796046e-01 -2.01869130e-01\n",
      " -1.44029409e-01 -1.55753363e-02  1.13757595e-01  3.80804166e-02\n",
      "  2.97156572e-02 -5.07184006e-02 -5.68684265e-02  1.87692251e-02\n",
      "  6.00924529e-03 -1.55048236e-01  1.58397615e-01 -4.43422198e-02\n",
      " -5.06770238e-03  1.17326632e-01  2.02104542e-02  1.42434880e-01\n",
      " -2.07507372e-01  5.91215491e-03  4.65370156e-02 -1.99849531e-01\n",
      "  6.28908873e-02 -7.12847039e-02 -3.18723798e-01 -1.33614847e-02\n",
      "  1.33507237e-01  1.73459351e-01  4.67833988e-02  3.03049814e-02\n",
      " -1.50837936e-02  2.52297759e-01 -2.59723067e-01  2.39182323e-01\n",
      "  8.29466730e-02 -4.64160815e-02 -1.44613817e-01 -1.11777239e-01\n",
      "  1.78967081e-02  1.85513613e-03 -8.28980468e-03  1.10304371e-01\n",
      "  7.43696913e-02  1.22373015e-01 -1.70870841e-01 -3.54967527e-02\n",
      "  5.86500987e-02 -1.08274058e-01  5.30113690e-02 -2.15808488e-02\n",
      " -1.37026995e-01  1.64991226e-02 -1.15879439e-01 -1.26042858e-01\n",
      "  3.06562707e-03 -8.47058445e-02  1.37670580e-02 -1.83165014e-01\n",
      "  1.50627375e-01  2.04155430e-01  1.67823672e-01 -1.36826321e-01\n",
      "  1.65433377e-01  8.03104341e-02 -2.25258932e-01  1.72896609e-01\n",
      " -5.92103601e-02 -2.07413390e-01  1.00724541e-01 -9.54180025e-04\n",
      "  1.43589228e-01  1.80428214e-02 -1.36468053e-01 -8.80302861e-02\n",
      " -5.75596094e-02 -1.76996482e-03  1.08443918e-02  8.28672275e-02\n",
      "  1.18184108e-02  2.40233615e-01  2.33847544e-01  1.43284544e-01\n",
      "  3.37122299e-04  1.27763718e-01  1.67887747e-01 -3.88663746e-02\n",
      "  6.95095807e-02 -1.66466177e-01  9.30395648e-02 -9.84605849e-02\n",
      "  4.12493907e-02 -1.77510634e-01 -1.68803766e-01 -7.51383081e-02\n",
      "  8.09697285e-02 -6.51166961e-03  4.31629755e-02 -1.40673205e-01\n",
      " -5.44511154e-02 -1.46770149e-01 -9.81214121e-02  1.72208250e-02\n",
      " -2.31258437e-01 -6.25184104e-02  8.63483772e-02 -1.13864336e-03\n",
      " -6.59777671e-02  2.19604179e-01 -1.96663290e-02  1.02983095e-01\n",
      " -2.94725094e-02 -9.43152085e-02  9.81614143e-02 -3.32851931e-02\n",
      "  8.37357864e-02 -1.45241886e-01  2.36284882e-01  6.74500316e-03\n",
      " -1.34659708e-01  1.94653533e-02 -2.08441466e-01 -2.61576116e-01\n",
      "  8.17339718e-02  3.04376185e-02  5.02737947e-02  1.35499332e-02\n",
      "  8.31005424e-02  1.11459851e-01 -2.23207146e-01  1.20359376e-01\n",
      "  1.30182534e-01 -2.74023358e-02  2.47594342e-01  2.73546875e-01\n",
      "  8.80492926e-02  1.75953582e-01 -7.43283704e-02 -2.33955950e-01\n",
      " -7.40154088e-02  1.23135159e-02  6.65781498e-02  2.48994976e-01\n",
      "  4.99914512e-02  9.01955962e-02  1.18462652e-01 -2.25028679e-01\n",
      " -1.42453358e-01  1.66804390e-03 -9.11707878e-02  1.67214632e-01\n",
      " -1.45884141e-01 -1.36985108e-01 -4.99020182e-02  9.34282725e-04\n",
      " -9.76009108e-03  2.06945483e-02 -3.91600057e-02  1.40047669e-01\n",
      " -8.94353818e-03  2.39353329e-01  1.15591981e-01 -8.44156221e-02\n",
      "  3.95215712e-02 -2.19073277e-02  5.72222471e-02  8.20151716e-02\n",
      " -1.22824594e-01 -1.04002431e-01  8.07963237e-02  7.82226101e-02\n",
      " -8.71652551e-03 -8.47060457e-02  3.20009999e-02 -9.83016044e-02\n",
      "  1.14857286e-01  3.12139411e-02  3.31526324e-02 -9.23502147e-02\n",
      " -2.78944932e-02  3.12539339e-02  1.05525948e-01 -1.49502081e-03\n",
      " -2.21365914e-02  2.39617489e-02  1.88747123e-01  8.14062431e-02\n",
      "  1.13095082e-02 -8.54382068e-02 -9.04278159e-02 -8.32363889e-02\n",
      "  3.78671438e-02 -2.11571932e-01  8.47461596e-02  6.22867199e-04\n",
      "  3.19701619e-02  1.96674585e-01  6.46226257e-02 -1.14832453e-01\n",
      "  2.31856499e-02 -7.77629539e-02 -2.11930990e-01 -1.82615399e-01\n",
      "  1.48405433e-01 -2.57633198e-02 -3.30985971e-02  2.39385232e-01\n",
      " -1.69035167e-01 -1.03444569e-01 -8.18260945e-03  7.26851150e-02\n",
      " -5.25061376e-02 -3.85228693e-02 -2.94059683e-02  2.78032333e-01\n",
      " -1.54948652e-01  6.21788725e-02 -1.86777264e-02 -1.18408605e-01\n",
      " -1.67330459e-01  6.69578090e-02  9.95121375e-02  4.25728820e-02]\n"
     ]
    }
   ],
   "source": [
    "query_embedding = get_sentence_embedding(queries.iloc[0]['text'], finetuned_fasttext_model)\n",
    "document_embedding = get_sentence_embedding(documents.iloc[0]['text'], finetuned_fasttext_model)\n",
    "print(\"Query Embedding:\", query_embedding)\n",
    "print(\"Document Embedding:\", document_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Retrieval System\n",
    "\n",
    "Construct a retrieval system that:\n",
    "\n",
    "- Retrieves the top 10 documents for a given query.\n",
    "- Combines scores from TF-IDF, BM25, and word embeddings to rank documents. You can use TfidfVectorizer from sklearn.feature_extraction.text, and BM25Okapi from rank_bm25 to obtain the TFIDF and BM25 scores. Use cosine similarity to calculate the embedding scores. The overall score for a query-document pair is calculated as the weighted average of the TF-IDF, BM25, and embedding scores.\n",
    "- Allows for the utilization of three FastText models: \n",
    "    - (1) the FastText model you trained from scratch on the CISI dataset, \n",
    "    - (2) a pre-trained FastText model, and \n",
    "    - (3) the fine-tuned version of the pre-trained model using the CISI dataset.\n",
    "- Provides the flexibility to set weights for TF-IDF, BM25, and embedding scores to compute a combined score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Evaluation\n",
    "Evaluation is crucial in information retrieval to ensure that the system meets user expectations. In this part, you\n",
    "will assess the performance of your retrieval system under various configurations. This includes experimenting\n",
    "with different possibilities of combining FastText embeddings with traditional TF-IDF and BM25 methods, as\n",
    "well as using each of them in isolation.\n",
    "- Performance Metrics: Evaluate the efficacy of your retrieval system using the Mean Average Precision\n",
    "(MAP) metric.\n",
    "- Comparative Analysis: Contrast the performance metrics when employing the three FastText models:\n",
    "\n",
    "    (1) the FastText model trained from scratch on the CISI dataset, \n",
    "\n",
    "    (2) the pre-trained FastText model, and\n",
    "\n",
    "    (3) the fine-tuned FastText model. Analyze the potential reasons for observed differences or similarities in the results.\n",
    "\n",
    "- Experimentation: Experiment with different combinations of weights for TF-IDF, BM25, and embedding scores. Also, test the performance of each method in isolation. Report your observations and the MAP scores for each scenario."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daily",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
