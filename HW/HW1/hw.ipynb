{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS533 HW1\n",
    "In this assignment, you will implement an information retrieval system using a combination of traditional\n",
    "retrieval methods and word embeddings. Specifically, you will be integrating TF-IDF and BM25 ranking\n",
    "methods with FastText embeddings. \n",
    "\n",
    "- Your task involves three different cases for using FastText: \n",
    "\n",
    "    (1) training a FastText model from scratch on the CISI dataset, \n",
    "\n",
    "    (2) using a pre-trained FastText model, and \n",
    "\n",
    "    (3) fine-tuning a pre-trained FastText model on the CISI dataset. \n",
    "    \n",
    "- You will rank documents for the given queries based on a combination of their traditional retrieval scores and the similarity between their embedding vectors. The performance of your retrieval system will be evaluated using the Mean Average Precision (MAP) metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Dataset Preprocessing\n",
    "\n",
    "* Load the CISI dataset from the provided CSV files. The dataset consists of:\n",
    "    \n",
    "    Documents: Represented by document id, title, and text.\n",
    "\n",
    "    Queries: List of natural language questions represented by query ids and texts.\n",
    "    \n",
    "    Ground Truth: Contains relevance judgments for query-document pairs.\n",
    "\n",
    "* Preprocess the dataset. This may include tokenizing the text, removing stopwords and punctuation marks, and lowercasing the text. Explain your preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents:\n",
      "   Unnamed: 0  doc_id                                             title  \\\n",
      "0           0       1  18 Editions of the Dewey Decimal Classifications   \n",
      "1           1       2                                               NaN   \n",
      "2           2       3                                Two Kinds of Power   \n",
      "3           3       4        Systems Analysis of a University Library;    \n",
      "4           4       5                        A Library Management Game:   \n",
      "\n",
      "           author                                               text  \n",
      "0  Comaromi, J.P.     The present study is a history of the DEWEY...  \n",
      "1             NaN  This report is an analysis of 6300 acts of use...  \n",
      "2      Wilson, P.      The relationships between the organization...  \n",
      "3  Buckland, M.K.      The establishment of nine new universities...  \n",
      "4      Brophy, P.      Although the use of games in professional ...  \n",
      "\n",
      "Queries:\n",
      "   Unnamed: 0  query_id                                               text\n",
      "0           0         1  What problems and concerns are there in making...\n",
      "1           1         2  How can actually pertinent data, as opposed to...\n",
      "2           2         3  What is information science?  Give definitions...\n",
      "3           3         4  Image recognition and any other methods of aut...\n",
      "4           4         5  What special training will ordinary researcher...\n",
      "\n",
      "Ground Truth:\n",
      "   Unnamed: 0  query_id  doc_id\n",
      "0           0         1      28\n",
      "1           1         1      35\n",
      "2           2         1      38\n",
      "3           3         1      42\n",
      "4           4         1      43\n"
     ]
    }
   ],
   "source": [
    "# Define the paths\n",
    "base_path = os.getcwd()\n",
    "cisi_path = os.path.join(base_path, \"CISI/\")\n",
    "\n",
    "# Load the datasets\n",
    "documents = pd.read_csv(f\"{cisi_path}documents.csv\")\n",
    "queries = pd.read_csv(f\"{cisi_path}queries.csv\")\n",
    "ground_truth = pd.read_csv(f\"{cisi_path}ground_truth.csv\")\n",
    "\n",
    "# Print data summaries\n",
    "print(\"Documents:\")\n",
    "print(documents.head())\n",
    "print(\"\\nQueries:\")\n",
    "print(queries.head())\n",
    "print(\"\\nGround Truth:\")\n",
    "print(ground_truth.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 2: Word Embedding\n",
    "1.  Training FastText: \n",
    "    Use the gensim library to train a FastText model on the CISI dataset. Ensure that both the text from the queries and the documents are used for the training process. This approach will help the model learn the specific vocabulary and context present in the dataset.\n",
    "2. Using a Pre-trained FastText Model: \n",
    "    Instead of starting from scratch, you can leverage pre-existing knowledge by using a pre-trained FastText model. Download a pre-trained FastText model, such as “cc.en.300.bin” (English), from the FastText website or using the fasttext library. The pre-trained model can be loaded using the gensim library for direct use in your retrieval system.\n",
    "3. Fine-tuning a Pre-trained FastText Model: \n",
    "    Further improve a pre-trained FastText model by finetuning it on the CISI dataset. This involves continuing the training process using the text from both the queries and the documents, allowing the model to adapt and better capture the characteristics and domain-specific vocabulary of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Retrieval Task\n",
    "In this part, you will focus on the core of information retrieval: fetching relevant documents based on a user's\n",
    "query. The challenge lies in effectively combining traditional retrieval methods with modern word embeddings\n",
    "to enhance the accuracy of the results.\n",
    "1. Embedding Computation: \n",
    "    Implement a method to derive the embedding for an input (a query or a document) by averaging the embeddings of its constituent words.\n",
    "2. Retrieval System: Construct a retrieval system that:\n",
    "\n",
    "    - Retrieves the top 10 documents for a given query.\n",
    "    - Combines scores from TF-IDF, BM25, and word embeddings to rank documents. You can use TfidfVectorizer from sklearn.feature_extraction.text, and BM25Okapi from rank_bm25 to obtain the TFIDF and BM25 scores. Use cosine similarity to calculate the embedding scores. The overall score for a query-document pair is calculated as the weighted average of the TF-IDF, BM25, and embedding scores.\n",
    "    - Allows for the utilization of three FastText models: (1) the FastText model you trained from scratch on the CISI dataset, (2) a pre-trained FastText model, and (3) the fine-tuned version of the pre-trained model using the CISI dataset.\n",
    "    - Provides the flexibility to set weights for TF-IDF, BM25, and embedding scores to compute a combined score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Evaluation\n",
    "Evaluation is crucial in information retrieval to ensure that the system meets user expectations. In this part, you\n",
    "will assess the performance of your retrieval system under various configurations. This includes experimenting\n",
    "with different possibilities of combining FastText embeddings with traditional TF-IDF and BM25 methods, as\n",
    "well as using each of them in isolation.\n",
    "- Performance Metrics: Evaluate the efficacy of your retrieval system using the Mean Average Precision\n",
    "(MAP) metric.\n",
    "- Comparative Analysis: Contrast the performance metrics when employing the three FastText models:\n",
    "\n",
    "    (1) the FastText model trained from scratch on the CISI dataset, \n",
    "\n",
    "    (2) the pre-trained FastText model, and\n",
    "\n",
    "    (3) the fine-tuned FastText model. Analyze the potential reasons for observed differences or similarities in the results.\n",
    "\n",
    "- Experimentation: Experiment with different combinations of weights for TF-IDF, BM25, and embedding scores. Also, test the performance of each method in isolation. Report your observations and the MAP scores for each scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "presidio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
